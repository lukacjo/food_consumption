#!/usr/bin/env python
# coding: utf-8

# ##### <center>  <img src="https://images.squarespace-cdn.com/content/v1/551a19f8e4b0e8322a93850a/1602020393443-L6M0DGZK4C75DRNR7GZH/Title_Animation.gif"  width=600> [[1]](#藕r)
# 
# 
# 

# <h2 class="alert alert-block alert-danger">  
# Projekt ten przedstawia m贸j tok mylenia oraz opinie! Wersja z czyst analiz dostpna jest na:    
# </h2>

# ## Jako osoba kt贸ra pracowaa na kuchni i dla kt贸rej temat jedzenia jako cz kultury jest niezwykle fascynujcy, w tym notatniku zajm si analiz bazy danych z WHO na temat konsumpcji produkt贸w na wiecie. Moim planem jest odpowiedzie na pytania, jakie produkty s najpopularniejsze i na co to wskazuje, jakie s r贸偶nice midzy kobietami i m偶czyznami oraz jak te dane mo偶na by wykorzysta tworzc menu do restauracji. Myl, 偶e w tych danych jest wiele ciekawych informacji, kt贸re mo偶na wykorzysta do odpowiedzenia na wspomniane pytania jak i do pogbienie wiedzy na temat tred贸w konsumcyjnych w r贸偶nych rejonach wiata.
# ## mo偶e bardziej pojc w to 偶e czym r贸偶ni si otwarcie restauracji w Europie ni偶 na wiecie? 偶e czy r贸偶nica midzy kobietami i m偶czyznami by sprawiaa 偶e profilowanie restauracji pod pe ma sens? czy celowanie w og贸lnowiatowe gusta ma sens czy lepiej ic w lokalne? myle 偶e w to i na podstawie tego podsumowania i wnioski
# ###  Plan jest nastpujcy:
# 1. Sprawdz jak skonstruowana jest baza danych, jakie informacje i typy danych znajduj sie w kolumnach.
# 1. Z t wiedz pozbd si kolumn, kt贸re na pewno nie bd mi potrzebne no chyba, 偶e takich nie bdzie.
# 1. Nastpnie dokonam analizy, gdzie znajduj sie puste dane oraz gdzie jest najwiksze ryzyko ich znajdowania si, a potem pozbd sie ich.
# 1. Sprawdz z jakich kraj贸w i kontynent贸w jest najwicej danych, 偶eby oceni ich u偶ywalnoc w globalnej skali.
# 1. Kolejn rzecz bdzie wyciagniecie danych dla produkt贸w spo偶ywanych na caym wiecie, w Europie oraz to samo dla m偶czyzn i kobiet na wiecie i w Europie.
# 1. Na koniec podsumuj co udao si osign, jakie byy wyzwania, problemy, bdy, sukcesy.
# ### Taki jest plan ale co napotkam na tej drodze to si oka偶e. Licz na to 偶e bdzie ciekawie! 

# # <font  color='289C4E'>Spis treci:<font><a class='anchor' id='top'></a> 
# 1. [Wczytywanie danych](#hello)
# 1. [Usuwanie niepotrzebnych kolumn](#del_col)
# 1. [Ujednolicanie danych](#stand)
# 1. [Usuwanie duplikat贸w i pustych danych](#del_dup)
# 1. [Wyciganie informacji z kolumn Consumers_Mean, Consumers_Median, Total_Mean, Total_Median](#colinfo)
# 1. [Sprawdzanie z jakich kraj贸w pochodz dane](#kraje)
# 1. [Problem z kolumn AgeClass](#ageclass)
# 1. [Wykres: Ilo badanych na przestrzeni lat](#sub_over_time)
#     1. [Wnioski](#sub_over_time_wnio)
# 1. [Wykres: Ilo badanych dla poszczeg贸lnych kraj贸w](#country_sub)
#     1. [Wnioski](#country_sub_wnio)
# 1. [Tworzenie kolumn z kodami i nazwami kontynent贸w](#kont)
# 1. [Wykres: Procentowy udzia badanych patrzc na kontynent](#kont_sub)
#     1. [Wnioski](#kont_sub_wnio)
# 1. [Wykres: Najpopularniejsze produkty na wiecie](#food_world)
# 1. [Wykres: Najpopularniejsze produkty w Europie](#food_eu) 
# 1. [Wykresy: Por贸wnanie wykres贸w najpopularniejsze produkty na wiecie i w Europie](#food_world_eu)
#     1. [Wnioski](#food_world_eu_wnio)
# 1. [Wykresy: Najpopularniejsze produkty dla m偶czyzn i kobiet na wiecie](#food_world_gen) 
#     1. [Wnioski](#food_world_gen_wnio)
# 1. [Wykresy: Najpopularniejsze produkty dla m偶czyzn i kobiet w Europie](#food_eu_gen)    
#     1. [Wnioski](#food_eu_gen_wnio)    
# 1. [Podsumowanie](#Podsumowanie)    
# 1. [Mo偶liwe zakamania](#risk)
# 1. [殴r贸da](#藕r)   
#     

# In[102]:


# import bibliotek oraz ustawienia
import pandas as pd
import missingno as msno
import matplotlib.pyplot as plt
import pycountry_convert as pc
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go
import numpy as np
from dash import dcc, html, Input, Output
from jupyter_dash import JupyterDash
import plotly.io as pio
pio.renderers.default = "notebook_connected" # bez tego nie wyswietlaj mi sie wykresy kiedy eksportuje je do htmla
plt.style.use('fivethirtyeight')


# # Wczytywanie danych <a id="hello"></a>[&uarr;](#top) [[2]](#藕r)

# In[103]:


food = pd.read_csv("fullcifocoss.csv", on_bad_lines='skip', sep=';', skipinitialspace = True)  # wczytanie danych do dataframe pozbywajc sie rzd贸w, kt贸re maj za du偶o p贸l oraz spacji
pd.set_option('display.max_columns', None) # sprawiam, 偶e mo偶na przejrze wszystkie kolumny, poniewa偶 by deafault ilo wywietlanych kolumn jest ograniczona
food.head()


# # Usuwanie niepotrzebnych kolumn <a id="del_col"></a>[&uarr;](#top)

# In[104]:


food.drop(columns=food.loc[:, 'Consumers_P05':'Consumers_Standard_deviation'], inplace=True)
food.drop(columns=food.loc[:, 'Total_P05':'ExtBWValue'], inplace=True)
food = food.drop(['BW'], axis=1)
food.head(10)


# In[105]:


food.shape


# ### Sprawdz czy typy danych w kolumnach si zgadzaj

# In[106]:


food.dtypes


# ### Wszystkie kolumny majdobre typy danych

# ### Ze wzgld贸w estetycznych ustawi by dane miay dwie liczby po przecinku. Jest to moja preferencja, a w razie potrzeby wr贸cenie do ustawie deafaultych nie bdzie problematyczne.

# In[107]:


pd.set_option('display.float_format', lambda x: '%.2f' % x)
food.head()


# ### <a id="stand"></a>[&uarr;](#top) R贸wnie偶 rzuca mi si w oczy to 偶e female i male w kolumnie Gender zaczynaj si z maej litery a All z du偶ej. Przypomina mi to o tym 偶eby ujednolici stringi. Zrobi to dla wszystkich kolumn zawierajcych stringi poza FoodCode bo mogo by w przyszosci mie to znaczenie podczas wczytywania kod贸w.

# In[108]:


food.loc[:, 'Country'] = food.loc[:, 'Country'].str.title()
food.loc[:, 'FoodName'] = food.loc[:, 'FoodName'].str.title()
food.loc[:, 'AgeClass'] = food.loc[:, 'AgeClass'].str.title()
food.loc[:, 'SourceAgeClass'] = food.loc[:, 'SourceAgeClass'].str.title()
food.loc[:, 'Gender'] = food.loc[:, 'Gender'].str.title()
food.head()


# ### Wszystko bardzo adnie si udalo i mam ujednolicone stringi.

# # <a class="anchor" id="del_dup">Usuwanie duplikat贸w i pustych danych.</a> [&uarr;](#top)

# In[109]:


food.shape


# In[110]:


food = food.drop_duplicates()


# In[111]:


food.shape


# In[112]:


544686-519881


# ### Tym prostym sposobem pozbyem si 24805 duplikat贸w.

# ### Przy u偶yciu biblioteki missingno oraz metody isna sprawdzam, gdzie znajduj sie puste dane.
# 

# In[113]:


msno.bar(food)


# In[114]:


food.isna().sum()


# ### Zar贸wno na wykresie jak i w tabeli wida, 偶e FoodName i FoodCode maj puste wartoci, wiec zajmuj sie usuniciem tych rzd贸w. (na razie nie zajmuj sie innymi kolumnami z pustymi watociami, bo nie wiem czy bd u偶yteczne).

# In[115]:


null_data = food[food.isnull().any(axis=1)]
null_data.tail(5) # NaN w FoodCode wida dopiero na 50 ale dla wygody pozostaje default


# In[116]:


food.loc[527540]


# ### Teraz wida, 偶e zar贸wno w kolumnie FoodName i FoodCode puste dane s opisane jako NaN. Dodatkowo w kolumnie FoodCode widz, 偶e jest kod znaczco du偶szy od innych, kt贸re na razie widziaem i znajduje si on zawsze tam gdzie jest pusta warto dla FoodName. Na podstawie tego decyduj, 偶 dobrym rozwizaniem jest usunicie rzd贸w w kt贸rych FoodName ma wartoci NaN bo nawet je偶eli kody s dobre to bez wiedzy jaki produkt one oznaczj s one bezu偶yteczne. Nastpnie zobacz czy po tym nadal pozostan dugie kody i puste wartoci w FoodCode. Prawdopodobnym powodem pustych danych jest bednie podany kod produktu co skutkuje tym, 偶e nie ma r贸wnie偶 nazwy produktu ale to si oka偶e dalej.

# In[117]:


food = food.dropna(subset=['FoodName'])


# In[118]:


food.isna().sum()      


# ### Usuwanie pustych danych powiodo si. Poprzez usuwanie pustych danych z FoodName usnem te偶 przy okazji puste dane z FoodCode. Teraz chc sprawdzi czy dugo kodu miaa znaczenie. Zrobi to najpierw na przykadzie kodu kt贸ry widziaem czyli fa6adbfab52e8a77f23df411f59c2150 oraz sprawdzajc kody o dugoci wikszej ni偶 5 czyli standardowej dugoci, kt贸r widziaem. 

# In[119]:


food.loc[food['FoodCode'] == "fa6adbfab52e8a77f23df411f59c2150"]


# In[120]:


temp = food['FoodCode'].str.len() > 5
temp.value_counts()


# ### Wychodzi na to, 偶e pozbyem si kodu fa6adbfab52e8a77f23df411f59c2150, ale kody o dugoci wikszej ni偶 5 nadal istniej i jest ich znaczco mniej, wic sprawdz teraz czy one s poprawne, chocia偶 na razie wszystko wskazuje na to, 偶e nie powinno by z nimi problemu.

# In[121]:


checkpoint = food #tworz checkpoint 偶eby m贸c atwo wr贸ci do wersji przed sortowaniem
food['CodeLen'] = food['FoodCode'].str.len()
food.sort_values(by=['CodeLen'])


# In[122]:


temp = food.loc[food['FoodCode'] == "a93a0316b93a7c2af9305e90012af119"]
len(temp)


# ### Moje przypuszczenia zostay potwierdzone, puste dane nie s zale偶ne od dugoci kodu, wic w tych kwestiach nie ma o co si martwi. 

# ### <a id="colinfo"></a>[&uarr;](#top) Teraz zajm si kolumnami Consumers_Mean, Consumers_Median, Total_Mean, Total_Median. Jako 偶e nie mam legendy to nie wiem co dokadnie one znacz i zakadanie co znacz oraz wiara 偶e posiadajdobre wartoci mo偶e by zgubna

# In[123]:


food = checkpoint # powr贸t do checkpointa
food.head()


# ### martwi mnie kwestia 偶e Total_Median, Consumers_Median maj wartoci 0,kiedy mediana nie powinna w takim przypadku mic zerowych wartoci

# In[124]:


food['Consumers_Mean'].loc[food['Consumers_Mean'] == 0].count() ,food['Consumers_Median'].loc[food['Consumers_Median'] == 0].count(),food['Total_Mean'].loc[food['Total_Mean'] == 0].count(),food['Total_Median'].loc[food['Total_Median'] == 0].count()


# In[125]:


food['Consumers_Mean'].count()


# ### Total_median ma prawie 500 000 wartoci 0. Totalnie dyskfalifikuje to u偶ywalno u偶ywalno tej kolumny. podobnie z Total_mean, niezale偶nie co ona znaczy niemo偶liwe 偶eby a偶 w 1/4 wynik贸w miaa tym bardziej 偶e one wystpuj w rzedach kt贸re majdane. Dla mnie wyklucza to totalnie u偶ywalno tych kolumn. 

# ### Consumers_Mean i Consumers_Median majmniej zerowych wartoci ale nadal du偶o ale spr贸buj na przykadzie chin dla oat grain sprawdzi czy te wartoci mo偶e maj jaki sens.

# In[126]:


food_all = food.loc[food['Gender'] == "All"]
food_all_oat = food_all.loc[food_all['FoodName'] == "Oat Grain"]
food_all_oat_ch = food_all_oat.loc[food_all_oat['Country'] == "China"]
food_all_oat_ch


# ### Tutaj pojawio si pare ciekawych rzeczy. Po pierwse to to 偶 dane s podwojone dla wszystkich kolumn nie liczc Consumers_Mean, Consumers_Median, Total_Mean, Total_Median. Dodatkowo polowa danych z Consumers_median i total_mean ma wartoci NaN. Na tym przykadzie wida 偶e Consumers_Median i Total_Median sa dla mnie bezu偶ytecznymi kolumnami bo niekt贸rych przypadkach nie dadz mi w og贸le informacji wiec ju偶 teraz moge ustali ze ich si pozbywam wiec jedynie consumers mean mo偶e by u偶yteczne i to to sprawdz czy posiada sensowne wartoci. Tylko martwi mnie to 偶 dane s podowjone, a dane consumers mean dwa razy sr贸偶ne prawdopodobnie ta kolumna te偶 do wyrzucenia jest. 

# In[127]:


food.isna().sum()


# ### Czyli dochdz kolejne bezu偶yteczne dane w tych kolumnach.

# In[128]:


food_all_oat_ch['Consumers_Mean'].iloc[0] 


# In[129]:


1157/66172*100 # procent konsument贸w z caej puli badanych 


# In[130]:


66172/1157 #liczba badanych podzielona przez liczb konsument贸w


# In[131]:


(1.12+66172)/1157 # poczona liczba badanych podzielona przez liczb konsument贸w


# In[132]:


food_all_oat_ch['Consumers_Mean'].iloc[1:7].sum() # suma rednich dla rzd贸w 1-6


# In[133]:


food_all_oat_ch['Consumers_Mean'].iloc[1:7].mean() # rednia ze rednich


# In[134]:


food_all_oat_ch['Number_of_consumers'].iloc[:7].mean() # rednia z pierwszy 7 rzd贸w


# In[135]:


food_all_oat_ch['Number_of_consumers'].iloc[0]/food_all['Number_of_consumers'].iloc[1:7].sum() # warto dla all podzielona przez sume wartoci rzd贸w 1-6


# ### 呕adne obliczenia nie daja takiej wartoci jaka jest w kolumnie Consumers_Mean, wic albo s to jakie inne dane, np. rednia ilo gram贸w spo偶ywanego produktu przez ankietowanych albo co zupenie innego, ale bez odpowiedniej wiedzy nie mo偶na tego zao偶y. Znaczy to, 偶e tych kolumn te偶 trzeba sie pozby, gdy偶 nawet gdyby byy pomocne, mog one zawiera faszywe wartoci.
# ### Usuwam wic: Consumers_Mean, Consumers_Median, Total_Mean, Total_Median i dodatkowo CodeLen kt贸re i tak ju偶 nie bdzie u偶yteczne dla mnie.

# In[136]:


food.shape


# In[137]:


food = food.drop(['Consumers_Mean', 'Consumers_Median', 'Total_Mean', 'Total_Median', 'CodeLen'], axis=1)
food.shape


# ### Wszystko poszo dobrze, pozbyem si 5 kolumn, wic teraz czas na usuwanie duplikat贸w, zrobi to za pomoc drop.duplicates bo tak jak widziaem dane byy podwojone i jedyne kolumny kt贸re uniemo偶liwiay usunicie duplikat贸w przy wczesniejszym przywoaniu dropduplicates wiec samo to powinno bozbyc si niepotrzebnych danych

# In[138]:


food.shape


# In[139]:


food = food.drop_duplicates()
food.shape


# In[140]:


544032-272016


# ### Tak jak wczesniej zauwa偶yem dane byy podwojone w caej bazie danych, wic bardzo dobrze, 偶e to zauwa偶yem, bo inaczej mogo by to mocno zakama wyniki.
# ### <a id="kraje"></a>[&uarr;](#top) Sprawdzam z jakich kraj贸w s dane dla wszystkich pci

# In[141]:


food_all = food.loc[food['Gender'] == "All"]
food_fem = food.loc[food['Gender'] == "Female"]
food_men = food.loc[food['Gender'] == "Male"]


# ### jeszcze przed tym dla upewnienia si sprawdze czy sumuj si dobrze rzdy czy czego nie straciem podczas przypisywania

# In[142]:


food_all['Country'].count() + food_fem['Country'].count() + food_men['Country'].count() - food['Country'].count()


# ### takie same iloci wic super

# In[143]:


food['Country'].nunique()


# In[144]:


food_all['Country'].nunique()


# In[145]:


food_fem['Country'].nunique()


# In[146]:


food_men['Country'].nunique()


# ### Niestety dane dla m偶czyzn s z mniejszej iloci kraj贸w, wic nale偶y wzi to pod uwag przy analizie.
# ### Sprawdz jakie kraje s zawarte w tych danych, czy all. fem maj takie same kraje oraz jakich kraj贸w nie ma w men.

# In[147]:


food_all['Country'].unique()


# In[148]:


food_fem['Country'].unique()


# In[149]:


food_men['Country'].unique()


# In[150]:


np.setxor1d(food_fem['Country'].unique(), food_all['Country'].unique()) # u偶ywam setxor1d z biblioteki numpy 偶eby sprawdzi czy te same kraje s dla fem i all


# In[151]:


np.setxor1d(food_fem['Country'].unique(), food_men['Country'].unique()) # sprawdzam jakich kraj贸w nie posiadaj m偶czy藕ni


# ### Jako 偶e jestem przy temacie kraj贸w to sprawdz czy jest Polska tutaj

# In[152]:


food_all.loc[food_all['Country'] == "Poland"]


# ### Niestety nie ma jej wic wnioski bd wyciga dla Europy.

# ### <a id="ageclass"></a>[&uarr;](#top) Bd chcia dane sprawdza dla wszystkich group wiekowych bez rozdzielania dokadnie na grupy wic sprawdz czy gdy tak filtruje to wszystko jest dobrze.

# In[153]:


food_all_all = food_all.loc[food_all['AgeClass'] == "All"]
food_fem_all = food_fem.loc[food_fem['AgeClass'] == "All"]
food_men_all = food_men.loc[food_men['AgeClass'] == "All"]
food_all_all.head()


# In[154]:


food_all_all['Country'].nunique()


# In[155]:


food_fem_all['Country'].nunique()


# In[156]:


food_men_all['Country'].nunique()


# ### O i tutaj jest du偶y problem przy takim rozdzieleniu iloci kraj贸w s inne ni偶 wczesniej sprawdzaem czyli musz znale藕 inne rozwizanie bo wynika z tego 偶e ageclass all nie jest dla wszystkich kraj贸w, niekt贸re kraje nie maj tego zgrupowanego

# In[157]:


np.setxor1d(food_all['Country'].unique(), food_all_all['Country'].unique())


# ### tak wstpnie patrzc to wydzhodi na to 偶e z Europy kraje nie mj ageclass All. Na przykadzie Francji sprawdz jak to dokadnie wyglda.

# In[158]:


food_fr = food.loc[food['Country'] == "France"] 
food_fr['AgeClass'].unique()
food_fr


# In[159]:


food_fr_wine = food_fr.loc[food_fr['FoodName'] == "Soft Drink, Flavoured With Herbs"] 
food_fr_wine.sort_values(by='SourceAgeClass')


# ### oznacza to 偶e musz znale藕 inny sposob na przedstawienie danych dla wszystkich grup wiekowych. rozwizaniem mo偶e by nieu偶ywanie ageclass all a za to samemu grupowa dane dla danego produktu wyko偶ystujc to 偶e ka偶de badanie ma unikaln ilo badanych, jest tu ryzko 偶e madania bd miay taka sam iloc badanych ale na razie nie widz innej opcji
# ### jako 偶enie bd rodziela danych na konkretne grupy wiekowe to sama ich suma mi wystarczy wiec pozbycie sie all i zsumowanie wszystkich wartoci robic to na podstawie tego 偶e ka偶de badanie ma inn iloc badanych to nie powinno by tu problemu ale upewni si
# ### wida 偶e dane female+male=All wic wszystko si pod tym wzgledem zgadza oznacza to 偶e przy rodzielaniu na pcie wszytko idzie dobrze. 

# In[160]:


food_all['Number_of_subjects'].nunique()


# ### znaczy to ze jest grupujc tak dane bd z 243 bada brane czyli jest to dobra pr贸ba badanych. mo偶e znajd lepszy sposob ale na razie wydaje sie to by najlepsze. 
# ### sprawdz jeszcze na przykadzie chin dla oat grain czy wszystkie wartoci dodane do siebie daj t sam warto co dla Ageclass All. 

# In[161]:


food_all_oat = food_all.loc[food_all['FoodName'] == "Oat Grain"]
food_all_oat_ch = food_all_oat.loc[food_all_oat['Country'] == "China"]

food_all_oat_ch


# In[162]:


food_all_oat_ch['Number_of_consumers'].iloc[1:].sum()-food_all_oat_ch['Number_of_consumers'].iloc[0]


# ### wynik jest 0 wic znaczy to 偶e sumy s dobre. Czyli podsumowujc moim pomysem jest usunicie rzd贸w z ageclass all i potem samodzielnei grupowa dane wykorzstujc zale偶noc 偶e badania maj r贸偶ne iloci badanych. czyli teraz usuwam rzdy z ageclass all i wstpnie sprawdz czy to dobrze dziaa

# In[163]:


food_fr = food_fr.loc[food_fr['Gender'] == "All"] 
food_fr['Number_of_subjects'].unique().sum() # chyba to bdzie rozwizaniem


# In[164]:


food_ctry = food_all.groupby(['Country','Number_of_subjects']).sum().reset_index()
food_ctry = food_ctry.groupby(['Country']).sum().reset_index()
food_ctry.loc[food_ctry['Country'] == "France"]


# In[165]:


food_ctry['Country'].nunique()


# ### wychodzi na to 偶e ta metoda daj dobr ilo badnaych wczsniej zakdana i nic nei trace w ten spos贸b a ilo kraj贸w jest dobra

# In[166]:


sub_over_time = food_all.groupby(['Year', 'Number_of_subjects']).sum().reset_index()
sub_over_time = sub_over_time.groupby(['Year']).sum().reset_index()
sub_over_time['Number_of_subjects'].sum()


# In[167]:


mask = food['AgeClass']== 'All'
food = food[~mask]


# In[168]:


food_all = food.loc[food['Gender'] == "All"]
food_fem = food.loc[food['Gender'] == "Female"]
food_men = food.loc[food['Gender'] == "Male"]


# In[169]:


food_all.count() + food_fem.count() + food_men.count() - food.count() 


# ### wychodzi 0 czyli wszystko jest dobrze i nie ma strat

# In[170]:


food_all['Country'].nunique()


# In[171]:


food_fem['Country'].nunique()


# In[172]:


food_men['Country'].nunique()


# ### iloci kraj贸w si zgadzaj
# ### chyba wszystkie dane s wyczyszczone i uporzdkowane 偶eby m贸c robiwykresy, wiem jak osign najbardziej zbli偶ony do rzeczywistoci wynik wiec wiec wszystko powinno by fine
# ### sprawdzam przy jakim grupowaniu suma Number_of_subjects jest najwieksza i z tej bd ko偶ystac. tak samo bd robi przy innych wykresach

# In[173]:


sub_over_time = food_all.groupby(['Year', 'Number_of_subjects','SourceAgeClass']).sum().reset_index()
sub_over_time = sub_over_time.groupby(['Year']).sum().reset_index()
sub_over_time['Number_of_subjects'].sum()


# #  Wykres: Ilo badanych na przestrzeni lat.<a id="sub_over_time"></a> [&uarr;](#top)

# In[174]:


hello = px.line(sub_over_time, x="Year", y="Number_of_subjects", title="over time", labels={'Number_of_subjects':'Number of Subjects', 'FoodName': 'Food Names' } , hover_data={'Number_of_subjects':':,'})
hello.show()


# # Wnioski <a id="sub_over_time_wnio"></a> [&uarr;](#top)

# ### Jak wida dane s g贸wnie z 202 i 2010. Najwyra藕niej wtedy byo albo najwieksze badanie albo najwiecej bada. 

# In[175]:


food_2002 = food_all.loc[food_all['Year'] == 2002]
food_2002 = food_2002.groupby(['Number_of_subjects','SourceAgeClass']).sum().reset_index()
food_2002['Number_of_subjects'].unique()


# In[176]:


food_2010 = food_all.loc[food_all['Year'] == 2010]
food_2010 = food_2010.groupby(['Number_of_subjects','SourceAgeClass']).sum().reset_index()
food_2010['Number_of_subjects'].unique()


# ### jak wida g贸wnie tutaj miay znaczenie du偶e badania ale r贸wnie偶 byo troche tych bada wiec jest to zawsze plus bo zwiksza to wiarygodno i zmiejsza bd z bada(jak to ianczej nazwa?)

# In[177]:


food_ctry = food_all.groupby(['Country', 'Number_of_subjects','SourceAgeClass','AgeClass']).sum().reset_index()
food_ctry = food_ctry.groupby(['Country', 'Number_of_subjects','SourceAgeClass']).sum().reset_index()
food_ctry = food_ctry.groupby(['Country']).sum().reset_index().sort_values(by='Number_of_subjects', ascending=False)

food_ctry['Number_of_subjects'].sum()


# #  Wykres: Ilo badanych dla poszczeg贸lnych kraj贸w.<a id="country_sub"></a>[&uarr;](#top)

# In[178]:


#testowanie dwustronnego suwaka

app = JupyterDash(__name__)
server = app.server
app.layout = html.Div([
    dcc.Graph(id='graph-with-slider'),
    dcc.RangeSlider(
        min=0,
        max=food_ctry['Number_of_subjects'].count(), # max i min wartoci mo偶liwe do wywietlenia
        step=1,
        id='my-range-slider',
		value=[0,food_ctry['Number_of_subjects'].count()], #wartoci wyswietlane na pocztku
    )
    
])

@app.callback( #porozumiewanie si graphu z suwakiem
    Output("graph-with-slider", "figure"), 
    Input('my-range-slider', 'value'))
def update_bar_chart(value): #funkcja updateujca wykres
    v1=value[0]
    v2=value[1] #dziki takiemu rozdzieleniu mog przekaza wartoci do iloc
    dff = food_ctry.iloc[v1:v2] # wybieram z jakiego przedziau dane maj si pokazywa po nr rzd贸w
    fig = px.bar(dff, x="Country", y="Number_of_subjects", title="Number of subjects per country", labels={'Number_of_subjects':'Number of subjects', 'Country': 'Countries' },hover_data={'Number_of_subjects':':,'})
    fig.update_xaxes(tickangle=40) #pochylenie nazw na x 偶eby byo atwiej czyta 
    return fig
if __name__ == '__main__':
    app.run_server(mode='inline', port="8005") #mode inline po to 偶eby wyswietlao si w notebooku, a nie poza. Port dla ka偶dego wykresu korzystajcego z JupyterDash bdzie inny bo 
    #inaczej pokazuje si ten sam wykres gdziekolwiek by u偶yty JupyterDash


# ### <a id="country_sub_wnio"></a>[&uarr;](#top) Na wykresie bardzo dobrze wida jak du偶a jest dysproporcja, co do iloci os贸b w zale偶noci od kraju. Znaczy to, 偶e wycignite wnioski mog by zachwiane i nale偶y o tym pamita podczas analizy.
# ### Dla dokadnoci sprawdz jak du偶a jest ta dysproporcja i jak rozkada si to pod wzgldem kontynent贸w.

# In[179]:


top5 = food_ctry['Number_of_subjects'].iloc[0:5].sum()
top5


# In[180]:


rest = food_ctry['Number_of_subjects'].iloc[5:].sum()
rest


# In[181]:


top5/rest


# ### Jak wida top 5 kraj贸w ma 1.65 razy wicej badanych ni偶 reszta kraj贸w, jest to bardzo du偶a dysporporcja, lecz nie dyskredytuje to od razu analizy wszystkich danych.
# # Tworzenie kolumny z kodami i nazwami kontynent贸w. <a id="kont"></a>[&uarr;](#top)
# ### Rozbicie na kontynenty, pozwoli zobaczy jak u偶yteczna bdzie globalna analiza i czy nie lepiej bdzie analizowa, ka偶dy kontynent odrbnie. U偶yj do tego biblioteki pycountry_convert.

# In[182]:


def convert(row): # funkcja przypisujca kod kontynentu w zale偶noci od kraju
    cn_code = pc.country_name_to_country_alpha2(row.Country, cn_name_format="default")
    conti_code = pc.country_alpha2_to_continent_code(cn_code)
    return conti_code


# ### Trzeba zamieni nazwy kilku kraj贸w, bo biblioteka pycountry_convert korzysta z innych nazw kraj贸w ni偶 te kt贸re s w dataframe.

# In[183]:


continent = food
ctry_change = {
	'Republic Of Korea' : 'South Korea',
    'Bolivia (Plurinational State Of)' : 'Bolivia',
    'United States Of America' : 'United States of America',
    "Lao People'S Democratic Republic" : "Lao People's Democratic Republic",
    "Democratic Republic Of The Congo" : "Democratic Republic of the Congo"
}
continent = continent.replace(ctry_change)


# In[184]:


continent['ContinentCode'] = continent.apply(convert, axis=1)
continent


# ### Mam kody kontynent贸w, wic niby mo偶na by tak to zostawi ale uwa偶am, 偶e du偶o adniej i czytelniej jest jak bd te偶 widoczne nazwy kontynent贸w.

# In[185]:


continent['ContinentCode'].unique()


# In[186]:


conti_names = {	# stworzenie sownika dla kontynent贸w, 偶eby m贸c zamieni kody kontynent贸w na nazwy kontynent贸w
				'AS' : 'Asia',
				'EU' : 'Europe',
                'NA' : 'North America',
                'SA' : 'South America',
                'AF' : 'Africa'
                }
continent['Continent'] = continent['ContinentCode'].map(conti_names)
continent


# In[187]:


continent['Continent'].unique()


# ### Jak wida wszystko adnie si udao, wic mog przypisa continent do food i ponownie stworzy dataframes dla ka偶dej pci.

# In[188]:


food = continent

food_all = food.loc[food['Gender'] == "All"]
food_fem = food.loc[food['Gender'] == "Female"]
food_men = food.loc[food['Gender'] == "Male"]


# In[189]:


food_con = food_all.groupby(['Country','SourceAgeClass','ContinentCode','Number_of_subjects']).sum().reset_index()
food_con = food_con[['ContinentCode','Number_of_subjects']].sort_values(by='Number_of_subjects', ascending=False)

food_con.sum()


# ### Ilo subject贸w jest taka jak wczesniej czyli 312217 wiec super

# ###  Niestety kontynenty te偶 s zgrupowane, wic musz odcia wszystko poza pierwszymi literami kodu, co pozwoli to dobrze podsumowa.
# 

# In[190]:


food_con['ContinentCode'] = food_con['ContinentCode'].apply(lambda x: x[0:2])
food_con = food_con.groupby(['ContinentCode']).sum().reset_index().sort_values(by='Number_of_subjects', ascending=False)
food_con


# ### Wszystko poszo dobrze, wic mog teraz wizualizowa, tylko jeszcze dodam nazwy kontynent贸w.

# In[191]:


food_con['Continent'] = food_con['ContinentCode'].map(conti_names)
food_con


# #  Wykres: Procentowy udzia badanych patrzc na kontynent.<a id="kont_sub"></a> [&uarr;](#top)

# In[192]:


fig = px.pie(food_con, values='Number_of_subjects', names='Continent', title='Percent of subjects per Continents',color_discrete_sequence=px.colors.sequential.RdBu, hover_data={'Number_of_subjects':':,'})
fig.show()


# ### <a id="kont_sub_wnio"></a> [&uarr;](#top) Na tym wykresie wida, ze rozo偶enie badancyh midzy kontynentami nie jest takie ze. Wiadomo Afryka najgorzej wypada i gdyby chcie wycign informacje dla Afryki, to mo偶na to robi tylko dla Afryki i nie sugerowa si og贸lnymi wynikami. Natomiast reszta kontynent贸w nawet r贸wno si rozkada, nadal dla dokadnych informacji nale偶y sprawdza konkretne kontynenty ale i tak ju偶 og贸lna analiza mo偶e da sensowne informacje.

# ### Teraz czas na sprawdzenie najpopularniejszych produkt贸w

# In[193]:


most_consumed_all = food_all.groupby(['FoodName']).sum().reset_index().sort_values(by='Number_of_consumers', ascending=False)
most_consumed_fem = food_fem.groupby(['FoodName']).sum().reset_index().sort_values(by='Number_of_consumers', ascending=False)
most_consumed_men = food_men.groupby(['FoodName']).sum().reset_index().sort_values(by='Number_of_consumers', ascending=False)
most_consumed_all[['FoodName','Number_of_consumers']].head(10)


# #  Wykres: Najpopularniejsze produkty na wiecie. <a id="food_world"></a>[&uarr;](#top)

# In[194]:


app = JupyterDash(__name__)
server = app.server
app.layout = html.Div([
    dcc.Graph(id='graph-with-slider'),
    dcc.RangeSlider(
        min=0,
        max=75, 
        step=1,
        id='my-range-slider',
		value=[0,30], 
    )
    
])

@app.callback( 
    Output("graph-with-slider", "figure"), 
    Input('my-range-slider', 'value'))
def update_bar_chart(value):
    v1=value[0]
    v2=value[1] 
    dff = most_consumed_all.iloc[v1:v2] 
    fig = px.bar(dff, x="FoodName", y="Number_of_consumers", 
       	title="Most popular foods in the World", 
       	labels={'Number_of_consumers':'Number of comsumers', 'FoodName': 'Food Names' },
      	color_discrete_sequence=["green"],
        hover_data={'Number_of_consumers':':,'})
    fig.update_xaxes(tickangle=40) 
    return fig
if __name__ == '__main__':
    app.run_server(mode='inline', port="8006") 


# #  Wykres: Najpopularniejsze produkty w Europie. <a id="food_eu"></a>[&uarr;](#top)

# In[195]:


food_all = food.loc[food['Gender'] == "All"]
food_all_eu = food_all.loc[food_all['Continent'] == "Europe"]
most_consumed_all_eu = food_all_eu.groupby(['FoodName']).sum().reset_index().sort_values(by='Number_of_consumers', ascending=False)


# In[196]:


app = JupyterDash(__name__)
server = app.server
app.layout = html.Div([
    dcc.Graph(id='graph-with-slider'),
    dcc.RangeSlider(
        min=0,
        max=75, 
        step=1,
        id='my-range-slider',
		value=[0,30], 
    )
    
])

@app.callback( 
    Output("graph-with-slider", "figure"), 
    Input('my-range-slider', 'value'))
def update_bar_chart(value): 
    v1=value[0]
    v2=value[1] 
    dff = most_consumed_all_eu.iloc[v1:v2] 
    fig = px.bar(dff, x="FoodName", y="Number_of_consumers", 
       title="Most popular food in Europe", 
       labels={'Number_of_consumers':'Number of comsumers', 'FoodName': 'Food Names' },
      	color_discrete_sequence=["blue"],
        hover_data={'Number_of_consumers':':,'})
    fig.update_xaxes(tickangle=40)
    return fig
if __name__ == '__main__':
    app.run_server(mode='inline',port="8015") 


# ### Dla wygody por贸wnywania dodamn te wykresy obok siebie.

# #  Wykresy: Por贸wnanie wykres贸w najpopularniejsze produkty na wiecie i w Europie <a id="food_world_eu"></a>[&uarr;](#top)

# In[197]:


app = JupyterDash(__name__)
server = app.server
app.layout = html.Div([
    dcc.Graph(id='graph-with-slider'),
    dcc.RangeSlider(
        min=0,
        max=75, 
        step=1,
        id='my-range-slider',
		value=[0,30], 
    )
    
])

@app.callback( 
    Output("graph-with-slider", "figure"), 
    Input('my-range-slider', 'value'))
def update_bar_chart(value): 
    v1=value[0]
    v2=value[1] 
    dff_world = most_consumed_all.iloc[v1:v2] 
    dff_eu = most_consumed_all_eu.iloc[v1:v2]
    first_line = go.Bar(x=dff_eu["FoodName"], y=dff_eu["Number_of_consumers"], name="Europe", marker=dict(color='blue'), hovertemplate = 'Number of consumers=%{y:,}')
    second_line = go.Bar(x=dff_world["FoodName"], y=dff_world["Number_of_consumers"], name="World", marker=dict(color='green'), hovertemplate = 'Number of consumers=%{y:,}')
    fig = make_subplots(rows=1, cols=2)
    fig.add_trace(first_line,row=1, col=1)
    fig.add_trace(second_line,row=1, col=2)
    fig.update_layout(title_text="Most popular foods for Europe and World")
    fig.update_xaxes(tickangle=40) 
    
    return fig
if __name__ == '__main__':
    app.run_server(mode='inline', port="8007") 


# ### <a id="food_world_eu_wnio"></a>[&uarr;](#top) Por贸wnujc dwa wykresy wida, 偶e Europa zamiast patk贸w niadaniowych na pierszym miejscu ma wod z kranu. Mo偶e to by poczone z wy偶szym bezpieczestwem wody z kranu w Europie [[3]](#藕r). Nastpnie nale偶y sie zastanowi nad wysokim miejscem pieczywa biaego, cebuli, marchewki, masa, kurczaka, pomidor贸w, czosnku, oliwy z oliwek i mleka. Powodem mo偶e by du偶y wpyw Francji i Woch na kuchni Europy.
# ### Mleko kt贸re te偶 jak wida czciej jest spo偶ywane w Europie ni偶 w reszcie wiata, co mo偶e wynika z mniejszej nietoleracji laktozy w Europie [[5]](#藕r). Wysokie miejsce margaryny, soli i cukru mo偶e tumaczy czste zachorowania na wysokie cinienie ttnicze[[4]](#藕r).

# #  Wykresy: Najpopularniejsze produkty u m偶czyzn i kobiet. <a id="food_world_gen"></a>[&uarr;](#top)

# In[198]:


app = JupyterDash(__name__)
server = app.server
app.layout = html.Div([
    dcc.Graph(id='graph-with-slider'),
    dcc.RangeSlider(
        min=0,
        max=75, 
        step=1,
        id='my-range-slider',
		value=[0,30], 
    )
    
])

@app.callback(
    Output("graph-with-slider", "figure"), 
    Input('my-range-slider', 'value'))
def update_bar_chart(value): 
    v1=value[0]
    v2=value[1] 
    dff_men = most_consumed_men.iloc[v1:v2] 
    dff_fem = most_consumed_fem.iloc[v1:v2]
    first_line = go.Bar(x=dff_men["FoodName"], y=dff_men["Number_of_consumers"], name="Male", hovertemplate = 'Number of consumers=%{y:,}')
    second_line = go.Bar(x=dff_fem["FoodName"], y=dff_fem["Number_of_consumers"], name="Female", hovertemplate = 'Number of consumers=%{y:,}')
    fig = make_subplots(rows=1, cols=2)
    fig.add_trace(first_line,row=1, col=1)
    fig.add_trace(second_line,row=1, col=2)
    fig.update_layout(title_text="Most popular foods for men and women")
    fig.update_xaxes(tickangle=40) 
    return fig
if __name__ == '__main__':
    app.run_server(mode='inline',port="8008") 


# ### <a id="food_world_gen_wnio"></a>[&uarr;](#top) Nie ma du偶ej r贸偶nicy midzy pciami jedynie kobiety majna wy偶szym miejscu patki niadaniowe oraz warzywa i ry偶, a natomiast m偶czy藕ni majna wy偶szym miejscu bia mk, biae pieczywo i miso wieprzowe.

# # Wykresy: najpopularniejsze produkty dla m偶czyzn i kobiet w Europie. <a id="food_eu_gen"></a>[&uarr;](#top)

# In[199]:


food_fem_eu = food_fem.loc[food_fem['Continent'] == "Europe"]
food_men_eu = food_men.loc[food_men['Continent'] == "Europe"]
most_consumed_fem_eu = food_fem_eu.groupby(['FoodName']).sum().reset_index().sort_values(by='Number_of_consumers', ascending=False)
most_consumed_men_eu = food_men_eu.groupby(['FoodName']).sum().reset_index().sort_values(by='Number_of_consumers', ascending=False)

app = JupyterDash(__name__)
server = app.server
app.layout = html.Div([
    dcc.Graph(id='graph-with-slider'),
    dcc.RangeSlider(
        min=0,
        max=75, 
        step=1,
        id='my-range-slider',
		value=[0,30], 
    )
    
])

@app.callback( 
    Output("graph-with-slider", "figure"), 
    Input('my-range-slider', 'value'))
def update_bar_chart(value): #funkcja updateujca wykres
    v1=value[0]
    v2=value[1] 
    dff_men = most_consumed_men_eu.iloc[v1:v2] 
    dff_fem = most_consumed_fem_eu.iloc[v1:v2]
    first_line = go.Bar(x=dff_men["FoodName"], y=dff_men["Number_of_consumers"], name="Male", hovertemplate = 'Number of consumers=%{y:,}')
    second_line = go.Bar(x=dff_fem["FoodName"], y=dff_fem["Number_of_consumers"], name="Female", hovertemplate = 'Number of consumers=%{y:,}')
    fig = make_subplots(rows=1, cols=2)
    fig.add_trace(first_line,row=1, col=1)
    fig.add_trace(second_line,row=1, col=2)
    fig.update_layout(title_text="Most popular foods for men and women in EU")
    fig.update_xaxes(tickangle=40) 
    return fig
if __name__ == '__main__':
    app.run_server(mode='inline',port="8009") 


# ### <a id="food_eu_gen_wnio"></a>[&uarr;](#top) W Europie podobnie jak na wiecie mae r贸偶nice midzy pciami. Jedynie m偶czy藕ni spo偶ywaj wiecej misa kurczaka oraz soli. Najwiksza r贸偶nica jest w przypadku misa wieprzowego, u kobiet wypada z top15 a u m偶czyzn zajmuje 10 miejsce. Kobiety jedynie spo偶ywaj wicej cukru.

# # Podsumowanie  <a id="Podsumowanie"></a>[&uarr;](#top)
# ### Jak wida nawet w bazach danych branych z renomowanych 藕r贸de znajduj sie puste dane, bdy i niejasnoci. To jak du偶 cz tego notatnika zajmowao czyszczenie danych i obchodzenie problem贸w pokazuje jak przydatna jest czysta i dobrze zbudowana baza danych.
# ### Z uporzdkowanych danych dao si wywnioskowa to, 偶e: 
# 1. Na wiecie patki niadaniowe, miso wieprzowe, ry偶 s bardziej popularne ni偶 w Europie
# 1. W Europie bardziej popularne ni偶 na wiecie s miso z kurczaka, mleko, ry偶, margaryna, oliwa z oliwek, banany, maso
# 1. Woda z kranu jest popularniejsza w Europie ni偶 na wiecie
# 1. Nie ma wielu r贸偶nic midzy pciami je偶eli chodzi o spo偶ywane produkty
# 1. M偶czy藕ni w Europie spo偶ywaj wiecej misa wieprzowego oraz soli od kobiet
# 1. Kobiety w Europie spo偶ywaj wicej cukru od m偶czyzn
# ### Mam nadziej, 偶e bya to przyjemna lektura i pokazaa ciekawe zale偶noci w wiecie jedzenia. Do zobaczenia.

# # Mo偶liwe zakamania: 锔<a id="risk"></a>[&uarr;](#top)
# 1. Zao偶enie, 偶e ka偶de badanie ma r贸偶n ilo badanych i w ten spos贸b grupowanie badanych.
# 1. Brak grupy wiekowej All dla wszystkich kraj贸w.
# 1. Wasne opinie i przewiadczenia.
# 1. Brak legendy do bazy danych.
# 1. Dane m偶czyzn s z mniejszej iloci kraj贸w.
# 1. Brak Polski i sprawdzanie dlanych dla Europy

# # 殴r贸da:  [&uarr;](#top) <a id="藕r"></a>
# 1. Gif: https://www.slynyrd.com/blog/2020/9/30/pixelblog-30-food 
# 1. Baza danych: https://apps.who.int/foscollab/Download/DownloadConso
# 1. Dane na temat jakoci wody na wiecie: https://worldpopulationreview.com/country-rankings/water-quality-by-country
# 	1. https://vividmaps.com/tap-water-safe-to-drink/
# 1. Badania na temat wpywu soli i cukru na cinienie ttnicze: https://sci-hub.se/10.1007/s00424-014-1677-x
# 	1. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4896734/
#     1. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6770596/
# 1. Dane na temat nietolerancji laktozy na wiecie: https://worldpopulationreview.com/country-rankings/lactose-intolerance-by-country
